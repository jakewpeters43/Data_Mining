---
title: "Homework11_Peters_Jake"
author: "Jake Peters"
date: "3/29/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dslabs)
library(caret)
library(ISLR2)
library(splines) #used for Regression Splines
library(tree) # for decision trees
library(ipred) # for bagging
library(rpart) # for tree-based methods
library(gbm) # for boosting
library(randomForest) # for random forest tree improvement
```
## Homework 

An education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let's simulate a dataset for 100 schools. First, let's simulate the number of students in each school.

```{r}
set.seed(1986)
n <- round(2^rnorm(100, 8, 1))
```

Now let's assign a _true_ quality for each school completely independent from size. This is the parameter we want to estimate. 

```{r}
mu <- round(80 + 2 * rt(100, 5))
range(mu)
schools <- data.frame(id = paste("PS",1:100), 
                      size = n, 
                      quality = mu,
                      rank = rank(-mu))
```

We can see that the top 10 schools are: 

```{r}
schools %>% top_n(10, quality) %>% arrange(desc(quality))
```

Now let's have the students in the school take a test. There is random variability in test taking so we will simulate the test scores as normally distributed with the average determined by the school quality and standard deviations of 30 percentage points:

```{r}
scores <- sapply(1:nrow(schools), function(i){
  scores <- rnorm(schools$size[i], schools$quality[i], 30)
  scores
})
schools <- schools %>% mutate(score = sapply(scores, mean))
```

1. What are the top schools based on the average score? Show just the ID, size, and the average score.

```{r}
schools %>% group_by(id) %>% summarize(size, score) %>% arrange(desc(score))

```

2. Compare the median school size to the median school size of the top 10 schools based on the score.

```{r}
median(schools$size)
top10 <- schools %>% arrange(desc(score)) %>% head(10) 
median(top10$size)


```

3. According to this test, it appears small schools are better than large schools. Five out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size are independent. Repeat the exercises 1 and 2 for the worst 10 schools.

```{r}

schools %>% group_by(id) %>% summarize(size, score) %>% arrange((score))
bottom10 <- schools %>% arrange((score)) %>% head(10) 
median(schools$size)
median(bottom10$size)


```


4. The same is true for the worst schools! They are small as well. Plot the average score versus school size to see what's going on. Highlight the top 10 schools based on the _true_ quality. Use the log scale transform for the size.

```{r}
basic <- ggplot(data = schools) + geom_point(aes(x= size, y = score)) +
  scale_x_log10()
# filter dataframe to get data to be highligheted
top10 <- schools %>% arrange(desc(score)) %>% head(10)

schools %>% 
  ggplot(aes(x=size,y=score)) + 
  geom_point() +
  geom_point(data=top10, 
             aes(x=size,y=score), 
             color='red',
             size=3) + scale_x_log10()



```

5. We can see that the score has larger variability when the school is smaller. This is a basic statistical reality. In fact, note that 4 of the top 10 schools are in the top 10 schools based on the exam score.

Let's use regularization to pick the best schools. Remember regularization _shrinks_ deviations from the average towards 0. So to apply regularization here, we first need to define the overall average for all schools:

```{r}
overall <- mean(sapply(scores, mean))
overall
```

and then define, for each school, how it deviates from that average. Write code that estimates the score above average for each school but dividing by $n + \lambda$ instead of $n$, with $n$ the school size and $\lambda$ a regularization parameter. Try $\lambda = 3$.

```{r}


reg_mean = function(v){
                    sum(v-overall)/(3 + length(v))
}
reg_mean_list <- sapply(scores, reg_mean) %>% as.data.frame()


schools <- schools %>% mutate(reg_scores = reg_mean_list[,1])
schools <- schools %>% mutate(adjusted_scores = overall + reg_scores)


tibble(original = schools$score, 
       regularlized = schools$adjusted_scores, 
       n = schools$size) %>%
  ggplot(aes(original, regularlized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5)
```


6. Notice that this improves things a bit. The number of small schools that are not highly ranked is now 4.  Is there a better $\lambda$? Find the $\lambda$ that minimizes the MSE = $1/100 \sum_{i=1}^{100} (\mbox{quality} - \mbox{estimate})^2$.

```{r}
lambdas <- seq(0,200,5)
MSE <- double(length(lambdas))
for (i in 1:length(lambdas)) {
  reg_mean = function(v){
                    sum(v-overall)/(lambdas[i] + length(v))
  }
   regscore <-  sapply(scores, reg_mean) + overall
    MSE[i] <- mean((schools$quality-regscore)^2)
}

which.min(MSE)
plot(MSE)
lambdas[17]
```
```{r}
reg_mean = function(v){
                    sum(v-overall)/(80 + length(v))
}
reg_mean_list <- sapply(scores, reg_mean) %>% as.data.frame()


schools <- schools %>% mutate(reg_scores = reg_mean_list[,1])
schools <- schools %>% mutate(adjusted_scores = overall + reg_scores)
tibble(original = schools$score, 
       regularlized = schools$adjusted_scores, 
       n = schools$size) %>%
  ggplot(aes(original, regularlized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5)
```



7. Rank the schools based on the average obtained with the best $\lambda$. Note that no small school is incorrectly included.

```{r}
reg_mean = function(v){
                    sum(v-overall)/(80 + length(v))
}
reg_mean_list <- sapply(scores, reg_mean) %>% as.data.frame()


schools <- schools %>% mutate(reg_scores = reg_mean_list[,1])
schools <- schools %>% mutate(adjusted_scores = overall + reg_scores)


top10correct <- schools %>% top_n(10, adjusted_scores) %>% arrange(desc(adjusted_scores))
top10correct

```


8.  A common mistake to make when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don't subtract the overall average before shrinking, we actually don't obtain a very similar result. Confirm this by re-running the code from exercise 6 but without removing the overall mean. 

```{r}

lambdas <- seq(0,200,5)
MSE <- double(length(lambdas))
for (i in 1:length(lambdas)) {
  reg_mean = function(v){
                    sum(v)/(lambdas[i] + length(v))
  }
   regscore <-  sapply(scores, reg_mean) + overall
    MSE[i] <- mean((schools$quality-regscore)^2)
}

which.min(MSE)
plot(MSE)
lambdas[17]
```

# Homework


## Spline Excercises

Using the "Wage" Dataset with age as x, wage as y, find what knots are best for it, and graph a spline using the bs() method.

```{r}
as_tibble(Wage)
splineAuto <- lm(Wage$wage~bs(Wage$age, knots = c(25,55,75)))
predicted_df <- data.frame(mpg_pred = predict(splineAuto, Wage), age= Wage$age)
ggplot(Wage) + 
  geom_point(mapping = aes(y = wage, x = age)) + 
  geom_line(color = "red", data = predicted_df, aes(y = mpg_pred, x = age))

summary(splineAuto)

```

Graph a spline using the ns method using the "Wage Dataset", how many degrees of freedom seems to look the best.
```{r}
as_tibble(Wage)
splineAuto <- lm(Wage$wage~ns(Wage$age, df = 8))
predicted_df <- data.frame(mpg_pred = predict(splineAuto, Wage), age= Wage$age)
ggplot(Wage) + 
  geom_point(mapping = aes(y = wage, x = age)) + 
  geom_line(color = "red", data = predicted_df, aes(y = mpg_pred, x = age))

summary(splineAuto)

```


Finally, using the smooth spline method, make a spline with how many degrees of freedom would be necessary, than use cross validation to see how many degrees of freedom the cv suggests
```{r}
autoSS <- smooth.spline(Wage$age, Wage$wage, cv = TRUE) # uses cv to give best lambda
plot(Wage$age, Wage$wage) + lines(autoSS)
autoSS

predicted_smoothSplineCV <- predict(autoSS,Wage$age)
postResample(predicted_smoothSplineCV$y,Wage$wage)

```

## Tree Exercises

```{r}
FM_housing = read_csv(file = "https://raw.githubusercontent.com/gmtanner-cord/DATA318/master/Original%20Data/FM_housing.csv",
                      col_types = cols(City = "f",
                                       `Book Section` = "f",
                                       `State/Province` = "f",
                                       `Postal Code` = "f",
                                       `Style` = "f",
                                       `Garage Type` = "f",
                                       `Flood Plain` = "f",
                                       Lake = "f",
                                       `Master Bedroom Main Flr` ="f",
                                       `Laundry Location` = "f",
                                       `High School` = "f")) %>%
  filter(!(`High School` %in% c("Central Cass","Barnesville","Horace"))) %>%
  mutate(`High School` = fct_drop(`High School`)) %>%
 dplyr::select(`Sold Date`, `List Price`, `Sold Price`, City, `State/Province`, `Postal Code`,
         `Geo Lat`, `Geo Lon`, `Total SqFt.`, `Year Built`, `Book Section`, Style, `Total Bedrooms`,
         contains("Bath"), `Garage Type`,`Gen Tax`, `Flood Plain`, `Master Bedroom Main Flr`, Lake,
         `Laundry Location`, contains("Area"),contains("SqFt"),`High School`,`Days on Market`) %>%
  dplyr::select(-contains("Per/SqFt")) %>%
  dplyr::select(-`List Price`, -`Gen Tax`, -`Sold Date`) %>%
  rename_all(funs(make.names(.)))# Makes the names of the dataset Tree friendly (all spaces are replaced with .)

set.seed(42) # This is for reproducibility, so that everyone gets the same answers.
test_index <- createDataPartition(FM_housing$Sold.Price, p = 0.20, list = FALSE)
test_set <- FM_housing[test_index,]
train_set <- FM_housing[-test_index,]
```

### Question 1
Create a regression tree of the FM housing training set to determine Sold Price using all the predictors. How many predictors end up actually being used? How many terminal nodes are there? What is the deviance?
```{r}

train <- sample(1:nrow(FM_housing), nrow(FM_housing) / 2)

tree <- tree(Sold.Price ~ ., data = train_set,mindev = 0.01) #code for the "tree" function is tree(formula, data)
plot(tree) #plots the decision tree
text(tree, pretty = 0) #adds labels to the plotted tree
tree




```
There are 10 terminal nodes. 4 predictors end up being used. The deviance is 1.3*10^13.
### Question 2
Use cross validation to determine how many terminal nodes is the most accurate for this tree. Use the smallest deviance value to determine the number of terminal nodes. How many terminal nodes is optimal? Does the tree require pruning? If so, prune the tree using the prune.tree() function with "best" being the optimal number of nodes.
```{r}
tree.cv <- cv.tree(tree) # the cross validation function for trees is simple and only requires the tree you wish to cross validate
plot(tree.cv$size, tree.cv$dev, type = "b")
tree.cv
tree.prune <- prune.tree(tree, best =10) #no pruning needed
plot(tree.prune)
text(tree.prune, pretty = 0)
summary(tree.prune)
```
10 terminal nodes is optimal. We dont need to prune.
### Question 3
Once you have a tree with the optimal number of nodes, use the predict() and postResample() functions to determine the RMSE. What is the RMSE to 2 decimal places?
(The arguments for postResample are postResample(predicted values, test_set$Sold.Price))
```{r}

yhat <- predict(tree.prune, newdata = test_set)
postResample(yhat, test_set$Sold.Price)
```
The RMSE is 5.42 * 10^4
### Question 4
Now, create a new tree using randomForest() using the same formula as the original tree and have mtry = 5 (throw in "na.action = na.omit" after "importance = TRUE" otherwise R will throw a fit). Then do the same thing you did in the previous question and use predict() and postResample() to determine the effectiveness of the new tree. What is the RMSE to 2 decimal places? Is the regular Tree or the randomForest Tree better?
```{r}
ranFor <- randomForest(Sold.Price ~ ., data = train_set, mtry = 5, importance = TRUE, na.action= na.omit)
yhat.rf <- predict(ranFor, newdata = test_set)

postResample(yhat.rf, test_set$Sold.Price)

```
The RMSE is 4.5 * 10^4. This is better than the regular Tree, and also better than the best lasso model! So the random forest is doing a really good job here!
For comparison, the best lasso model have a RMSE of 5.15e^04 