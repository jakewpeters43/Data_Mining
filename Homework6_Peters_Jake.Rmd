---
title: "Homework6_Peters_Jake"
author: "Jake Peters"
date: "2/15/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(ggplot2)
library(tidyverse)
library(caret)
```
Answer questions #6 and #13 (a-d) from 4.8
6.a)
```{r}
# .05 * Hours studied + 1 * undergrad gpa - 6
 # p(X) = exp(B0 + B1X1 + B2X2) / (1 + exp( b0 + b1x1 + b2))

exp(-6 + .05*40 + 1 *3.5) / (1 + exp(-6 + .05*40 + 1 *3.5))


```
37.7 % chance of getting an A in the class
6b)
```{r}

# 0.5 = exp(-6 + .05* X1 + 1 *3.5) / (1 + exp(-6 + .05*X1 + 1 *3.5))
# .5 = .5 * exp(-25 + .05X1)
# log(1) = -2.5 + .05X1
2.5/.05

```
50 hours need to be studied to get an A in the class

13a-d
a.
```{r}
Weekly

ggplot(Weekly) + geom_line(aes(x=Year, y = Lag1))
ggplot(Weekly) + geom_line(aes(x=Year, y = Today))
Weekly %>% count(Direction)
ggplot(Weekly) + geom_bar(aes(x=Direction))
ggplot(Weekly)+ geom_jitter(aes(x=Year, y=Volume ))
mean(as.numeric(Weekly$Lag1))
mean(as.numeric(Weekly$Lag2))
mean(as.numeric(Weekly$Lag3))
mean(as.numeric(Weekly$Lag4))
mean(as.numeric(Weekly$Lag5))
sum(as.numeric(Weekly$Lag1))
sum(as.numeric(Weekly$Lag2))
sum(as.numeric(Weekly$Lag3))
sum(as.numeric(Weekly$Lag4))
sum(as.numeric(Weekly$Lag5))

```
There were more Up weeks than down weeks, by a slight margin as we can see in the bar graph. Also, volume increased greatly as the years went on, with the data having heteroscedasticity as well, fanning out for more variability in volume later on.
b.
```{r}
mod1 <- glm(Direction ~ Volume + Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = Weekly, family = binomial)
summary(mod1)

glm.prob <- predict(mod1, type = "response")
glm.pred <- rep("Down", nrow(Weekly))
glm.pred[glm.prob > .5] = "Up"
```
Lag2 is the only variable that seems like it is statistically significant.
c.
```{r}
attach(Weekly)
confusionMatrix(factor(if_else(glm.prob <= 0.5, "Down", "Up"), levels = c("Down","Up")),Weekly$Direction, positive = "Down")

```
True positive rate: 557/(557+430) = 56.4
Sensitivity/recall = 557/(557 + 48) = 92
Specificity: 54/(430+54) = 11.2
The confusion matrix is telling me that the model is a little better than a coin flip at predicting when Down is positive
d.
```{r}
attach(Weekly)
train <- (Year < 2009)
test <- Weekly[!train,]

modtrain <- glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
modlog.prob <- predict(modtrain, test, type = "response")
confusionMatrix(factor(if_else(modlog.prob <= 0.5, "Down", "Up"), levels = c("Down","Up")),test$Direction, positive = "Down")


```
accuracy = 62.5
